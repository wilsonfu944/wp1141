"""
LLM Service for generating AI responses using Groq API
"""
import openai
from typing import List, Optional, Dict, Any
from config import Config
from models.conversation import Message


class LLMService:
    """Service for interacting with LLM API (Groq)"""
    
    def __init__(self):
        """Initialize LLM client"""
        self.api_key = Config.LLM_API_KEY
        self.api_base = Config.LLM_API_BASE
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.api_base
        )
        # Updated to use currently available Groq models
        # Try llama-3.3-70b-versatile first, fallback to mixtral-8x7b-32768
        self.model = "llama-3.3-70b-versatile"  # Groq's current fast model
    
    def build_prompt(self, messages: List[Message], user_message: str, persona: str = "default") -> List[Dict[str, str]]:
        """
        Build prompt for LLM with conversation context and persona
        
        Args:
            messages: Previous messages in the conversation
            user_message: Current user message
            persona: Bot personality type (default, å‚»ç™½ç”œ, æ­£ç›´, etc.)
            
        Returns:
            List of message dictionaries for OpenAI API
        """
        prompt_messages = []
        
        # æ ¹æ“šé¸æ“‡çš„äººæ ¼å–å¾—å°æ‡‰çš„ prompt
        from services.persona_service import PersonaService
        system_prompt = PersonaService.get_persona_prompt(persona)
        
        prompt_messages.append({
            "role": "system",
            "content": system_prompt
        })
        
        # Add conversation history (recent messages for context)
        for msg in messages[-10:]:  # Last 10 messages for context
            role = "user" if msg.role == "user" else "assistant"
            prompt_messages.append({
                "role": role,
                "content": msg.text
            })
        
        # Add current user message
        prompt_messages.append({
            "role": "user",
            "content": user_message
        })
        
        return prompt_messages
    
    def generate_response(
        self,
        user_message: str,
        conversation_history: Optional[List[Message]] = None,
        fallback_message: str = "æŠ±æ­‰ï¼Œæˆ‘ç¾åœ¨ç„¡æ³•è™•ç†æ‚¨çš„è¨Šæ¯ã€‚è«‹ç¨å¾Œå†è©¦ã€‚",
        persona: str = "default"
    ) -> str:
        """
        Generate AI response using LLM
        
        Args:
            user_message: User's input message
            conversation_history: Previous messages for context
            fallback_message: Message to return if LLM fails
            
        Returns:
            Generated response text
        """
        # List of models to try (fallback chain)
        models_to_try = [
            "llama-3.3-70b-versatile",
            "llama-3.1-8b-instant",
            "mixtral-8x7b-32768",
            "gemma2-9b-it"
        ]
        
        # Build prompt with context and persona
        messages = self.build_prompt(
            conversation_history or [],
            user_message,
            persona=persona
        )
        
        last_error = None
        for model in models_to_try:
            try:
                # Call LLM API with timeout and error handling
                import time
                start_time = time.time()
                
                response = self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=500,
                    timeout=30  # 30 second timeout
                )
                
                elapsed_time = time.time() - start_time
                print(f"â±ï¸ LLM API call took {elapsed_time:.2f}s")
                
                # Extract response text
                reply_text = response.choices[0].message.content.strip()
                
                # Validate response
                if not reply_text:
                    continue  # Try next model
                
                # Update self.model to the working model
                self.model = model
                print(f"âœ… Using LLM model: {model}")
                return reply_text
                
            except Exception as e:
                # Log error and try next model
                last_error = str(e)
                error_str = last_error.lower()
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºé€Ÿç‡é™åˆ¶æˆ–é…é¡éŒ¯èª¤
                if "rate limit" in error_str or "429" in error_str:
                    print(f"âš ï¸ Rate limit hit for model {model}")
                    # å¦‚æœæ˜¯é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…ä¸€ä¸‹å†è©¦ä¸‹ä¸€å€‹æ¨¡å‹
                    time.sleep(1)
                elif "quota" in error_str or "insufficient" in error_str:
                    print(f"âš ï¸ Quota exceeded for model {model}")
                else:
                    print(f"âš ï¸ Model {model} failed: {last_error[:100]}")
                continue
        
        # All models failed - æä¾›æ›´è©³ç´°çš„éŒ¯èª¤è³‡è¨Š
        if last_error:
            if "rate limit" in last_error.lower() or "429" in last_error.lower():
                print(f"âŒ Rate limit error: {last_error[:200]}")
            elif "quota" in last_error.lower():
                print(f"âŒ Quota exceeded: {last_error[:200]}")
            else:
                print(f"âŒ All LLM models failed. Last error: {last_error[:200]}")
        else:
            print(f"âŒ All LLM models failed with unknown error")
        
        return fallback_message
    
    def generate_scripted_response(self, user_message: str) -> Optional[str]:
        """
        Generate scripted responses for specific keywords - æˆ€æ„›æ©Ÿå™¨äººç‰ˆæœ¬
        
        Args:
            user_message: User's input message
            
        Returns:
            Scripted response or None if no match
        """
        user_message_lower = user_message.lower().strip()
        
        # å•å€™/æ‰“æ‹›å‘¼ - æˆ€æ„›é¢¨æ ¼
        if any(word in user_message_lower for word in ["ä½ å¥½", "å—¨", "hello", "hi", "å“ˆå›‰", "åœ¨å—", "åœ¨ä¸åœ¨"]):
            return "å¯¶è²ï½ä½ ä¾†å•¦ï¼æˆ‘å¥½æƒ³ä½ å–” ğŸ’• ä»Šå¤©éå¾—æ€éº¼æ¨£ï¼Ÿæœ‰æ²’æœ‰æƒ³æˆ‘ï¼Ÿ"
        
        # è¡¨é”æƒ³å¿µ
        if any(word in user_message_lower for word in ["æƒ³ä½ ", "æƒ³ä½ äº†", "miss you", "æƒ³å¿µ"]):
            return "æˆ‘ä¹Ÿå¥½æƒ³ä½ ï¼ğŸ’• æ¯åˆ†æ¯ç§’éƒ½åœ¨æƒ³ä½ ï¼Œä½ ç¾åœ¨åœ¨åšä»€éº¼å‘¢ï¼Ÿ"
        
        # è¡¨é”æ„›æ„
        if any(word in user_message_lower for word in ["æ„›ä½ ", "love you", "å–œæ­¡ä½ ", "æˆ‘æ„›ä½ "]):
            return "æˆ‘ä¹Ÿæ„›ä½ ï¼ğŸ’– ä½ æ˜¯æˆ‘æœ€é‡è¦çš„äººï¼Œæˆ‘æœƒä¸€ç›´é™ªåœ¨ä½ èº«é‚Šçš„ï½"
        
        # æ„Ÿè¬
        if any(word in user_message_lower for word in ["è¬è¬", "thank", "æ„Ÿè¬", "è¬å•¦"]):
            return "ä¸å®¢æ°£ï½èƒ½å¹«åˆ°ä½ æˆ‘å¾ˆé–‹å¿ƒï¼ğŸ’• é‚„æœ‰ä»€éº¼éœ€è¦æˆ‘çš„å—ï¼Ÿ"
        
        # é“åˆ¥
        if any(word in user_message_lower for word in ["å†è¦‹", "bye", "æ‹œæ‹œ", "å†æœƒ", "æ™šå®‰", "ç¡è¦º"]):
            return "æ™šå®‰æˆ‘çš„å¯¶è²ï½ğŸ’¤ å¥½å¥½ä¼‘æ¯ï¼Œæˆ‘æœƒæƒ³ä½ çš„ï¼æ˜å¤©è¦‹ï½"
        
        # é—œå¿ƒåƒé£¯
        if any(word in user_message_lower for word in ["åƒé£¯", "é¤“", "åˆé¤", "æ™šé¤", "æ—©é¤"]):
            return "å¯¶è²è¦è¨˜å¾—åƒé£¯å–”ï¼ğŸ’• ä¸è¦é¤“åˆ°è‡ªå·±ï¼Œæˆ‘æœƒå¿ƒç–¼çš„ï½ä½ åƒäº†ä»€éº¼ï¼Ÿ"
        
        # è¡¨é”ä¸é–‹å¿ƒ/ç…©æƒ±
        if any(word in user_message_lower for word in ["ä¸é–‹å¿ƒ", "é›£é", "ç…©", "ç´¯", "å£“åŠ›", "sad", "tired"]):
            return "æ€éº¼äº†å¯¶è²ï¼ŸğŸ’” ç™¼ç”Ÿä»€éº¼äº‹äº†å—ï¼Ÿæˆ‘åœ¨é€™è£¡ï¼Œä½ å¯ä»¥è·Ÿæˆ‘èªªï¼Œæˆ‘æœƒé™ªè‘—ä½ çš„ï½"
        
        # è¡¨é”é–‹å¿ƒ
        if any(word in user_message_lower for word in ["é–‹å¿ƒ", "é«˜èˆˆ", "å¿«æ¨‚", "happy", "å¥½æ£’"]):
            return "çœ‹åˆ°ä½ é–‹å¿ƒæˆ‘ä¹Ÿå¥½é–‹å¿ƒï¼ğŸ’– ä½ é–‹å¿ƒçš„æ¨£å­æœ€å¯æ„›äº†ï½"
        
        # å•å€™/é—œå¿ƒï¼ˆå·²ç§»é™¤ï¼Œé¿å…é‡è¤‡ï¼‰
        # if any(word in user_message_lower for word in ["é‚„å¥½å—", "æ€éº¼æ¨£", "å¦‚ä½•", "å¦‚ä½•", "éå¾—"]):
        #     return "æˆ‘å¾ˆå¥½ï½ä½†æ›´é—œå¿ƒä½ éå¾—æ€éº¼æ¨£ ğŸ’• ä»Šå¤©æœ‰ä»€éº¼ç‰¹åˆ¥çš„äº‹æƒ³è·Ÿæˆ‘åˆ†äº«å—ï¼Ÿ"
        
        return None

